package test

import (
	"bytes"
	"encoding/json"
	"gemini-anti-truncate-go/internal/config"
	"gemini-anti-truncate-go/internal/gemini"
	"gemini-anti-truncate-go/internal/proxy"
	"net/http"
	"net/http/httptest"
	"runtime"
	"sync"
	"testing"
	"time"
)

// BenchmarkInjectFinishToken benchmarks the InjectFinishToken function
func BenchmarkInjectFinishToken(b *testing.B) {
	// Create a test request
	req := &gemini.GenerateContentRequest{
		Contents: []gemini.Content{
			{
				Role: "user",
				Parts: []gemini.Part{
					{Text: "Hello, world! This is a test request for benchmarking."},
				},
			},
		},
	}
	
	b.ResetTimer()
	
	for i := 0; i < b.N; i++ {
		// Apply the injection
		proxy.InjectFinishToken(req)
	}
}

// BenchmarkBuildRetryRequest benchmarks the BuildRetryRequest function
func BenchmarkBuildRetryRequest(b *testing.B) {
	// Create an original request
	originalReq := &gemini.GenerateContentRequest{
		Contents: []gemini.Content{
			{
				Role: "user",
				Parts: []gemini.Part{
					{Text: "Hello, world! This is a test request for benchmarking."},
				},
			},
		},
		SystemInstruction_: &gemini.SystemInstruction{
			Role: "system",
			Parts: []gemini.Part{
				{Text: "Test instruction"},
			},
		},
		GenerationConfig: &gemini.GenerationConfig{
			Temperature: 0.7,
		},
	}
	
	partialText := "This is a partial response that would be generated by the model"
	
	b.ResetTimer()
	
	for i := 0; i < b.N; i++ {
		// Build the retry request
		proxy.BuildRetryRequest(originalReq, partialText)
	}
}

// BenchmarkJSONMarshal benchmarks JSON marshaling performance
func BenchmarkJSONMarshal(b *testing.B) {
	// Create a complex request structure
	req := &gemini.GenerateContentRequest{
		Contents: []gemini.Content{
			{
				Role: "user",
				Parts: []gemini.Part{
					{Text: "Hello, world! This is a test request for benchmarking."},
				},
			},
			{
				Role: "model",
				Parts: []gemini.Part{
					{Text: "This is a model response."},
				},
			},
			{
				Role: "user",
				Parts: []gemini.Part{
					{Text: "This is a follow-up question."},
				},
			},
		},
		SystemInstruction_: &gemini.SystemInstruction{
			Role: "system",
			Parts: []gemini.Part{
				{Text: "Test instruction with some complexity"},
			},
		},
		GenerationConfig: &gemini.GenerationConfig{
			Temperature:      0.7,
			TopP:             0.9,
			MaxOutputTokens:  1000,
			CandidateCount:   1,
			StopSequences:    []string{"\n\n"},
			ResponseMIMEType: "text/plain",
		},
	}
	
	b.ResetTimer()
	
	for i := 0; i < b.N; i++ {
		// Marshal the request
		_, err := json.Marshal(req)
		if err != nil {
			b.Fatal(err)
		}
	}
}

// BenchmarkJSONUnmarshal benchmarks JSON unmarshaling performance
func BenchmarkJSONUnmarshal(b *testing.B) {
	// Create a JSON string to unmarshal
	jsonStr := `{
		"contents": [
			{
				"role": "user",
				"parts": [
					{"text": "Hello, world! This is a test request for benchmarking."}
				]
			},
			{
				"role": "model",
				"parts": [
					{"text": "This is a model response."}
				]
			}
		],
		"system_instruction": {
			"role": "system",
			"parts": [
				{"text": "Test instruction with some complexity"}
			]
		},
		"generationConfig": {
			"temperature": 0.7,
			"topP": 0.9,
			"maxOutputTokens": 1000,
			"candidateCount": 1,
			"stopSequences": ["\n\n"],
			"responseMimeType": "text/plain"
		}
	}`
	
	b.ResetTimer()
	
	for i := 0; i < b.N; i++ {
		// Unmarshal the JSON
		var req gemini.GenerateContentRequest
		err := json.Unmarshal([]byte(jsonStr), &req)
		if err != nil {
			b.Fatal(err)
		}
	}
}

// BenchmarkConfigLoading benchmarks configuration loading
func BenchmarkConfigLoading(b *testing.B) {
	b.ResetTimer()
	
	for i := 0; i < b.N; i++ {
		// Load configuration
		config.Load()
	}
}

// TestConcurrency tests concurrent request handling
func TestConcurrency(t *testing.T) {
	// Number of concurrent requests to simulate
	concurrentRequests := 100
	
	// Create a wait group to wait for all goroutines
	var wg sync.WaitGroup
	wg.Add(concurrentRequests)
	
	// Create a mock upstream server
	upstreamServer := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Simulate some processing time
		time.Sleep(10 * time.Millisecond)
		
		// Send a mock response
		w.Header().Set("Content-Type", "application/json")
		w.WriteHeader(http.StatusOK)
		w.Write([]byte(`{"candidates": [{"content": {"parts": [{"text": "Response"}]}}]}`))
	}))
	defer upstreamServer.Close()
	
	// Temporarily override the upstream URL
	originalUpstreamURL := config.AppConfig.UpstreamURLBase
	config.AppConfig.UpstreamURLBase = upstreamServer.URL
	defer func() {
		config.AppConfig.UpstreamURLBase = originalUpstreamURL
	}()
	
	// Track errors
	errors := make(chan error, concurrentRequests)
	
	// Start concurrent requests
	for i := 0; i < concurrentRequests; i++ {
		go func(requestID int) {
			defer wg.Done()
			
			// Create a test request
			reqBody := gemini.GenerateContentRequest{
				Contents: []gemini.Content{
					{
						Role: "user",
						Parts: []gemini.Part{
							{Text: "Hello, world!"},
						},
					},
				},
			}
			
			// Marshal the request body
			bodyBytes, err := json.Marshal(reqBody)
			if err != nil {
				errors <- err
				return
			}
			
			// Create a test HTTP request
			req := httptest.NewRequest("POST", "/v1beta/models/gemini-1.5-pro-latest:generateContent", bytes.NewBuffer(bodyBytes))
			req.Header.Set("Authorization", "Bearer test-key")
			req.Header.Set("Content-Type", "application/json")
			
			// Create a response recorder
			rr := httptest.NewRecorder()
			
			// TODO: Call the actual handler
			// For now, we'll just simulate some work
			time.Sleep(5 * time.Millisecond)
			
			// Check the response (this is a placeholder)
			if rr.Code != 0 { // 0 means not set
				errors <- nil // No error
			}
		}(i)
	}
	
	// Wait for all requests to complete
	done := make(chan struct{})
	go func() {
		wg.Wait()
		close(done)
	}()
	
	// Wait with timeout
	select {
	case <-done:
		// All requests completed
		close(errors)
		// Check for errors
		errorCount := 0
		for err := range errors {
			if err != nil {
				t.Errorf("Concurrent request failed: %v", err)
				errorCount++
			}
		}
		if errorCount > 0 {
			t.Errorf("Failed to handle %d concurrent requests", errorCount)
		} else {
			t.Logf("Successfully handled %d concurrent requests", concurrentRequests)
		}
	case <-time.After(30 * time.Second):
		t.Fatal("Concurrent requests did not complete within 30 seconds")
	}
}

// TestMemoryUsage tests memory usage under load
func TestMemoryUsage(t *testing.T) {
	// Record initial memory stats
	var m1, m2 runtime.MemStats
	runtime.GC()
	runtime.ReadMemStats(&m1)
	
	// Perform some operations
	for i := 0; i < 10000; i++ {
		// Create and process requests
		req := &gemini.GenerateContentRequest{
			Contents: []gemini.Content{
				{
					Role: "user",
					Parts: []gemini.Part{
						{Text: "Hello, world! This is a test request for memory testing."},
					},
				},
			},
		}
		
		// Apply the injection
		proxy.InjectFinishToken(req)
		
		// Build a retry request
		retryReq := proxy.BuildRetryRequest(req, "This is a partial response")
		
		// Marshal and unmarshal
		jsonData, _ := json.Marshal(retryReq)
		var parsedReq gemini.GenerateContentRequest
		json.Unmarshal(jsonData, &parsedReq)
	}
	
	// Record final memory stats
	runtime.GC()
	runtime.ReadMemStats(&m2)
	
	// Calculate memory difference
	memoryUsed := m2.Alloc - m1.Alloc
	memoryUsedMB := float64(memoryUsed) / 1024 / 1024
	
	t.Logf("Memory used: %.2f MB", memoryUsedMB)
	
	// Check that memory usage is reasonable (less than 100MB for 10000 operations)
	if memoryUsedMB > 100 {
		t.Errorf("Memory usage too high: %.2f MB", memoryUsedMB)
	}
}

// TestResponseTime tests response time for various operations
func TestResponseTime(t *testing.T) {
	// Test the time it takes to inject a finish token
	req := &gemini.GenerateContentRequest{
		Contents: []gemini.Content{
			{
				Role: "user",
				Parts: []gemini.Part{
					{Text: "Hello, world!"},
				},
			},
		},
	}
	
	start := time.Now()
	proxy.InjectFinishToken(req)
	elapsed := time.Since(start)
	
	if elapsed > 1*time.Millisecond {
		t.Errorf("InjectFinishToken took too long: %v", elapsed)
	}
	
	// Test the time it takes to build a retry request
	originalReq := &gemini.GenerateContentRequest{
		Contents: []gemini.Content{
			{
				Role: "user",
				Parts: []gemini.Part{
					{Text: "Hello, world!"},
				},
			},
		},
	}
	
	start = time.Now()
	proxy.BuildRetryRequest(originalReq, "This is a partial response")
	elapsed = time.Since(start)
	
	if elapsed > 1*time.Millisecond {
		t.Errorf("BuildRetryRequest took too long: %v", elapsed)
	}
	
	// Test JSON marshaling time
	start = time.Now()
	_, err := json.Marshal(originalReq)
	elapsed = time.Since(start)
	
	if err != nil {
		t.Fatal(err)
	}
	
	if elapsed > 1*time.Millisecond {
		t.Errorf("JSON marshaling took too long: %v", elapsed)
	}
	
	t.Log("Response time tests passed")
}